# 🌟 Welcome, LLM Trailblazers! Let's Build Your LLMs Ground Up 🌟

This repository is dedicated to all Trailblazers embarking on the journey to build Large Language Models from the ground up and apply them to their projects. Here you will find a series of Jupyter notebooks that guide you through the process of building a Generative Pre-trained Transformer model from scratch.

## 🌳 Repository Structure

```
.
├── data/
├── helpers/
├── .gitignore
├─1_Setup.ipynb
├─2_Tokenization.ipynb
├─3_Attention.ipynb
├─4_GPT.ipynb
├─5_Training.ipynb
└── requirements.txt
```

## 🛠️ Notebooks and Flow

The notebooks are designed to be completed in order, each building on the concepts introduced in the previous ones:

| Notebook | Description | Open in Colab |
| -------- | ----------- | ------------- |
| 🏁 **Setup** | Introduction to the project, importing DistilGPT2 for a basic model. | [Open In Colab](https://colab.research.google.com/github/EliaLesyk/trailblazeGPT/blob/main/1_Setup.ipynb) |
| ✂️ **Tokenization** | Overview of tokenization techniques and custom dataloader implementation. | [Open In Colab](https://colab.research.google.com/github/EliaLesyk/trailblazeGPT/blob/main/2_Tokenization.ipynb) |
| 🧠 **Attention** | Deep dive into attention mechanisms, such as dot-product, scaled attention, and multi-head attention. | [Open In Colab](https://colab.research.google.com/github/EliaLesyk/trailblazeGPT/blob/main/3_Attention.ipynb) |
| 🏗️ **GPT Architecture** | Build the core GPT model, including Multi-Head Attention, Layer Normalization, Feed-Forward Neural Network, and Residual Connections. | [Open In Colab](https://colab.research.google.com/github/EliaLesyk/trailblazeGPT/blob/main/4_GPT.ipynb) |
| 🎓 **Training** | Train, evaluate, and experiment with hyperparameters for the GPT model. | [Open In Colab](https://colab.research.google.com/github/EliaLesyk/trailblazeGPT/blob/main/5_Training.ipynb) |


## 🎉 Get Started

Clone the repository and explore the notebooks to learn how to build and train your own LLMs!

Each notebook contains cells marked with `TODO`. These are points where you're encouraged to implement key components of the GPT architecture, helping to reinforce your understanding of how the model works.

To get the most out of this tutorial:
1. Clone the repository
2. Install the required dependencies (listed in `requirements.txt`)
3. Work through the notebooks in order, completing the `TODO` sections
4. Experiment with the code and hyperparameters to deepen your understanding

## Prerequisites

- Basic understanding of Python and PyTorch
- Familiarity with neural network concepts
- Jupyter Notebook environment

## Acknowledgments

This tutorial is designed to make understanding GPT accessible to a wider audience. While some mathematical concepts have been simplified, the core principles of the GPT architecture are preserved.

Happy learning, and enjoy building your own GPT model!

## Contributions

Contributions, issues, and feature requests are welcome! Feel free to check the issues page if you want to contribute.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.


## 🔗 Connect with Me

- [LinkedIn](https://www.linkedin.com/in/elina-lesyk/)
- [X (Twitter)](https://x.com/elesyk)
- [Website](https://elinalesyk.com/)

