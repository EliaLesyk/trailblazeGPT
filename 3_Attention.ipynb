{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b63376be-0049-4883-a8ed-33b75f687ffc",
   "metadata": {},
   "source": [
    "# Week 3: Attention, Please!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d926e6-a3f1-4d4c-ae85-43d45e0673b1",
   "metadata": {},
   "source": [
    "Attention mechanisms,  are a cornerstone of modern natural language processing and form the heart of GPT (Generative Pre-trained Transformer) models. \n",
    "\n",
    "We are building on top of [the paper introducing Transformer architecture by Vaswani et al](https://arxiv.org/abs/1706.03762). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf6f0d3-7386-4992-9280-837995bde2f7",
   "metadata": {},
   "source": [
    "#### What is Attention?\n",
    "Attention in neural networks allows the model to focus on specific parts of the input when producing an output. In the context of language models like GPT, attention enables the model to consider relevant words or phrases when predicting the next word, regardless of their position in the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ff4453-2136-4d1f-9490-bf79a3b31383",
   "metadata": {},
   "source": [
    "GPT models use a specific type of attention called \"self-attention\" within their transformer blocks. Attention fits into the GPT architecture through:\n",
    "\n",
    "1. Input Embedding: Words are converted into vectors.\n",
    "2. Positional Encoding: Position information is added to these vectors.\n",
    "3. Transformer Blocks: Multiple layers, each containing:\n",
    "    - Multi-Head Attention: Allows the model to attend to different aspects of the input simultaneously.\n",
    "    - Feed-Forward Neural Network: Processes the attention output.\n",
    "4. Output Layer: Produces probabilities for the next word.\n",
    "\n",
    "In this notebook, we'll implement and visualize these attention mechanisms, providing you with a deep understanding of how GPT models process and generate text.\n",
    "\n",
    "Let's begin our journey into the world of attention!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64acda99-1b72-4517-a43b-43613ce1d709",
   "metadata": {},
   "source": [
    "## 0. Query, Key, Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a831509-dad6-4620-b780-131a53e58e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import base64\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b50aa69-35b9-478a-8d64-80debb81aa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.check_todo import check_implementation\n",
    "from helpers.show_mermaid import mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a426d34-5021-4ccb-b223-af227400b98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the src directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs('src', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd189956-72fd-4a17-b8e0-5ac91c786e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define your example sentence\n",
    "sentence = \"Think of humanity on the path towards more unfathomable complexity\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b44ec2-de5d-44bc-be29-3db8c33990a1",
   "metadata": {},
   "source": [
    "Let's use a simplfied representation of this sentence where each element is either \"0\" or \"1\"with 3 dimensions. We could think of these 3 dimensions as representing very basic linguistic features. For example:\n",
    "1. First dimension could represent \"action\" or \"concreteness\"\n",
    "2. Second dimension: Could represent \"relation\" or \"abstractness\"\n",
    "3. Third dimension: Could represent \"complexity\" or \"plurality\".\n",
    "\n",
    "Thus, a word \"think\" can be represented as [1, 0, 0] since it is an action verb, concrete, singular, while \"of\" as [0, 1, 0] since it is a relational word, abstract, singular. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dde5db-b455-45bb-be53-2b6e6ef00d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_vec = {\n",
    "    \"Think\": [1, 0, 0],\n",
    "    \"of\": [0, 1, 0],\n",
    "    \"humanity\": [1, 0, 1],\n",
    "    \"on\": [0, 1, 1],\n",
    "    \"the\": [1, 1, 1],\n",
    "    \"path\": [1, 0, 0],\n",
    "    \"towards\": [1, 1, 0],\n",
    "    \"more\": [0, 0, 1],\n",
    "    \"unfathomable\": [1, 0, 1],\n",
    "    \"complexity\": [0, 1, 1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe635b7-257c-4440-b324-9f2c9d4c55bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, vector in word_to_vec.items():\n",
    "    print(f\"{word}: {vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c03969f-308c-4cb9-8329-f1e38ec970a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = sentence.split()\n",
    "query = torch.tensor([word_to_vec[\"path\"]], dtype=torch.float)  # We're querying with the specific word\n",
    "key = torch.tensor([word_to_vec[word] for word in words], dtype=torch.float)\n",
    "value = torch.tensor([[i] for i in range(len(words))], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5149605c-d807-4748-aea5-b09a62c72b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Query shape:\", query.shape)\n",
    "print(\"Key shape:\", key.shape)\n",
    "print(\"Value shape:\", value.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d55a8e3-cca7-459d-bdfa-67466fb506bb",
   "metadata": {},
   "source": [
    "The attention scores are computed by taking the dot product of the query vector with each key vector (word in the sentence in this case). The dot product is a measure of similarity. So in this simple representation:\n",
    "- Words with the same vector as \"path\" ([1, 0, 0]) will have a score of 1.\n",
    "- Words with completely different vectors will have a score of 0.\n",
    "- Words with some overlap will have a score between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5303db4c-91d0-4fe0-9f4b-833786022394",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = query @ key.T\n",
    "print(\"\\nAttention Scores:\", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4499f80-ff92-431b-8805-88cd7d8783cc",
   "metadata": {},
   "source": [
    "The softmax function turns these scores into a probability distribution. This step:\n",
    "1. Ensures all weights are between 0 and 1.\n",
    "2. Makes the weights sum to 1.\n",
    "3. Enhances the differences between scores, making high scores even higher and low scores lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dd4992-e9e3-41aa-8a98-5aca50ca08c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_weights = F.softmax(scores, dim=-1)\n",
    "print(\"Attention Weights:\", attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5497c4c5-6968-48db-bb9c-130e154cd6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention weights\n",
    "# Think what words contribute most to the query word's representation\n",
    "query_word = \"path\"\n",
    "\n",
    "plt.figure(figsize=(12, 2))\n",
    "plt.imshow(F.softmax(query @ key.T, dim=-1).detach(), cmap='viridis', aspect='auto')\n",
    "plt.title(f\"Attention Weights for Query: '{query_word}'\")\n",
    "plt.xticks(range(len(words)), words, rotation=45, ha='right')\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71425158-502b-4d32-a10a-d7bf4fcd8ac3",
   "metadata": {},
   "source": [
    "We multiply each value (which in this case is just the position of the word) by its corresponding attention weight and sum the results. In math terms this is matrix multiplication. The result represents a \"weighted average\" position in the sentence, where positions of words with higher attention weights contribute more to the final sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d577f7db-75c7-4192-b4d7-ee00762c297c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the step-by-step calculation for weighted average\n",
    "print(\"\\nStep-by-step calculation for a weghted sum:\")\n",
    "for i, (weight, val) in enumerate(zip(attention_weights[0], value)):\n",
    "    print(f\"Word {i}: {weight.item():.2f} * {val.item()} = {weight.item()*val.item():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe71a7f-f3e7-4a11-973d-6ca1a0b894fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = attention_weights @ value\n",
    "print(\"Weighted sum also referred to as a context vector:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c0d06e-6a22-43a4-84e8-1c0b2be6d16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mm(\"\"\"\n",
    "graph TD\n",
    "    A[Input Sequence] --> B[Query Q]\n",
    "    A --> C[Key K]\n",
    "    A --> D[Value V]\n",
    "    B --> E[Compute Attention Scores]\n",
    "    C --> E\n",
    "    E --> F[Apply Softmax]\n",
    "    F --> G[Weighted Sum]\n",
    "    D --> G\n",
    "    G --> H[Output]\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560bc19e-db77-4ea8-a2f0-fbc6aaf44f38",
   "metadata": {},
   "source": [
    "## 1. Dot-Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af864e95-60f1-4d56-9bd1-ecccbd1dbaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product_attention(query, key, value):\n",
    "    # TODO: Implement dot-product attention    \n",
    "    # Hint: Compute attention scores, apply softmax, and compute weighted sum\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a7373a-6b61-4862-880e-b1ce750289c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    check_implementation(dot_product_attention)\n",
    "except NotImplementedError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180327f3-597e-489c-83ff-8b85228bbe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = dot_product_attention(query, key, value)\n",
    "print(\"Dot-Product Attention Output:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659f0f60-15e8-4db9-86e2-eb1355a54e8b",
   "metadata": {},
   "source": [
    "## 2. Scaled Dot-Product Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982ed16f-a61a-4a53-8f17-933beb371f1f",
   "metadata": {},
   "source": [
    "In scaled attention, we introduce a scaling factor of a square root of the dimnesion of key vectors. You might ask why scaling is needed... There are at least 3 reasons:\n",
    "1. As the dimension of the key vectors increases, the dot products tend to grow larger in magnitude.\n",
    "2. Large dot products push the softmax function into regions where it has extremely small gradients.\n",
    "3. This can lead to very peaked (near-one-hot) distributions after softmax, or to vanishing gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b078ab-36df-4a5e-a5be-3441bbd63f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value):\n",
    "    # TODO: Implement scaled dot-product attention    \n",
    "    # Hint: Similar to dot-product attention, but scale the scores\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529b8e74-ec7f-47f4-a5d5-9adc047408d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    check_implementation(scaled_dot_product_attention)\n",
    "except NotImplementedError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0873b01-ec3b-4f9f-8fa9-394554e6a4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_scaled = scaled_dot_product_attention(query, key, value)\n",
    "print(\"Scaled Dot-Product Attention Output:\", output_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40956522-7c55-4cbd-9fd3-97630967f455",
   "metadata": {},
   "source": [
    "## 3. Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772b4b13-008c-4574-b0c2-1d3931da7d71",
   "metadata": {},
   "source": [
    "In self-attention, instead of using the input directly as query, key, and value, we create projections of the input. These projections allow the model to transform the input into different representation spaces (and different dimensions). \n",
    "\n",
    "Vector projection uses the dot product because the dot product provides a measure of how much one vector aligns with another, which is exactly what projection aims to capture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b31003-d507-49a7-a662-21afea13b607",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([word_to_vec[word] for word in words], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cda5d2e-1a51-488a-aa48-2942a16a2d49",
   "metadata": {},
   "source": [
    "We'd now review the concept of linear transfomation and `nn.Linear`. When we apply the linear layer, it performs the following operation for each sequence element:\n",
    "output = x @ weight.T + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0be5fb-20a1-443b-9d23-fef4af4e5506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a simple example\n",
    "input_dim = x.shape[1]\n",
    "out_dim = 3\n",
    "seq_length = 2\n",
    "batch_size = 1\n",
    "\n",
    "# Create an nn.Linear layer\n",
    "linear_layer = nn.Linear(input_dim, out_dim)\n",
    "\n",
    "# Print the weight matrix and bias\n",
    "print(\"Weight matrix shape:\", linear_layer.weight.shape)\n",
    "print(\"Weight matrix:\")\n",
    "print(linear_layer.weight.data)\n",
    "print(\"\\nBias shape:\", linear_layer.bias.shape)\n",
    "print(\"Bias:\")\n",
    "print(linear_layer.bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d408a506-84ce-4863-b2b7-e71c70667c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nInput shape:\", x.shape)\n",
    "print(\"Input:\")\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febe35d8-2551-48ad-9ff0-7c645980f105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the linear transformation\n",
    "output = linear_layer(x)\n",
    "\n",
    "print(\"\\nOutput shape:\", output.shape)\n",
    "print(\"Output:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd80fff6-fa38-46d9-b898-828fd16ec346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification with the dot product\n",
    "manual_output = torch.matmul(x, linear_layer.weight.t()) + linear_layer.bias\n",
    "print(\"\\nManual calculation output:\")\n",
    "print(manual_output)\n",
    "print(\"\\nDoes manual calculation match nn.Linear output?\", torch.allclose(output, manual_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55222b4-ee0a-413e-8bf7-8d0b0a4130b0",
   "metadata": {},
   "source": [
    "The key difference between this and regular scaled dot-product attention is the use of learned projections (Q, K, V) instead of using the input directly. This self-attention mechanism allows each position in the sequence to attend to all positions, including itself.\n",
    "The `nn.Linear` layers are crucial here:\n",
    "1. They introduce learnable parameters, allowing the model to adapt how it projects the input for attention computation.\n",
    "2. They enable the model to transform the input in ways that are most useful for the task at hand, which is learned during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083fcea8-ca7d-4a9f-a9e7-defe965c75c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleSelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim, out_dim):\n",
    "        super(SimpleSelfAttention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "        # Create linear projections for Q, K, V\n",
    "        self.q_proj = nn.Linear(input_dim, out_dim)\n",
    "        self.k_proj = nn.Linear(input_dim, out_dim)\n",
    "        self.v_proj = nn.Linear(input_dim, out_dim)\n",
    "        \n",
    "        self.fc_out = nn.Linear(out_dim, input_dim) # -> (batch_size, seq_length, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #  Implement SimpleSelfAttention class forward()\n",
    "        # Hint: Use self.q_proj, self.k_proj, and self.v_proj to create projections from the input\n",
    "        # Apply the scaled dot-product attention mechanism using these projections\n",
    "        # Use self.fc_out for the final projection\n",
    "\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa73b7c-d363-4959-b68b-3f1191c6edb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    check_implementation(SimpleSelfAttention)\n",
    "except NotImplementedError as e:\n",
    "    print(e)\n",
    "# even if you correctly implement forward function for this nn.Module you'll still get a warning\n",
    "# do you know why?\n",
    "# if not, go forward and check the next class implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163690f1-3715-43fd-b4de-40bf000e0f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_attention = SimpleSelfAttention(input_dim, out_dim)\n",
    "output, attention_weights = self_attention(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ed091d-4e59-4223-a37a-e4827e6c3b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention weights\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(attention_weights.detach().numpy(), cmap='viridis')  # Convert to numpy if needed\n",
    "plt.title(\"Self-Attention Weights\")\n",
    "plt.xlabel(\"Key\")\n",
    "plt.ylabel(\"Query\")\n",
    "plt.xticks(range(len(words)), words, rotation=45, ha='right')\n",
    "plt.yticks(range(len(words)), words)\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0638ff1-0b4b-4e94-9c99-255903339600",
   "metadata": {},
   "source": [
    "## 4. Causal Attention with Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d7ca02-702b-4e6c-b02f-60afed63c8d2",
   "metadata": {},
   "source": [
    "With GPT models, the autoregressive property is important to maintain. It refers to a characteristic of certain models, where the prediction of the current step or value depends only on the information from the previous steps or values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9c1628-6282-4c5f-86dc-b12490ddf176",
   "metadata": {},
   "source": [
    "Causal attention, also known as masked attention, is a specialized form of self-attention.\n",
    "\n",
    "It restricts a model to only consider previous and current inputs in a sequence when processing any given token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4b992d-5b6f-4ede-861d-104682d0a7bb",
   "metadata": {},
   "source": [
    "From the math perspective, `torch.triu` creates an upper triangular matrix from an input tensor with teh main diagonal and above. It's often used to create masks for causal attention in transformer models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d55b0e-cf27-4a71-ae56-96fd164542de",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = torch.ones(5, 5)\n",
    "print(\"Original matrix:\")\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325ff488-77eb-4e2f-8808-3b1a1f43e042",
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_triangular = torch.triu(matrix)\n",
    "print(\"\\nUpper triangular matrix:\")\n",
    "print(upper_triangular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09af3513-4615-4b07-8e37-305705db846a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Offset the diagonal\n",
    "offset_triangular = torch.triu(matrix, diagonal=1)\n",
    "print(\"\\nUpper triangular matrix with offset 1:\")\n",
    "print(offset_triangular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0524d7a-f58b-40e0-807e-1f46ef2662cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make masked values be ignored also after Softmax\n",
    "neg_inf = upper_triangular.masked_fill(upper_triangular == 0, float('-inf'))\n",
    "neg_inf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b981494-ec07-4182-b5ad-197b79a5380b",
   "metadata": {},
   "source": [
    "Any `-inf` value in scores will result in a zero probability after applying the Softmax function, meaning those positions are ignored. This effectively ensures that each token can only attend to itself and the tokens before it in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108b03b6-afed-4b7e-b8b1-f4d942b1fde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a boolean mask\n",
    "mask = torch.triu(matrix, diagonal=1).bool()\n",
    "print(\"\\nBoolean mask for upper triangular with offset 1:\")\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2018dc1-4ae9-4998-b0f8-2ae32a44bc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim, out_dim):\n",
    "        super(CausalSelfAttention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "        self.q_proj = nn.Linear(input_dim, out_dim)\n",
    "        self.k_proj = nn.Linear(input_dim, out_dim)\n",
    "        self.v_proj = nn.Linear(input_dim, out_dim)\n",
    "        \n",
    "        self.fc_out = nn.Linear(out_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "\n",
    "        # TODO: Implement CausalSelfAttention class forward pass\n",
    "        # Hint: 1. re-use SimpleSelfAttention\n",
    "        # 2. create causal mask using torch.triu over x \n",
    "        # 3. update mask zero values to -inf before Softmax\n",
    "\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e7929d-bb70-46ef-bf59-4bfeb9c43623",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    check_implementation(CausalSelfAttention)\n",
    "except NotImplementedError as e:\n",
    "    print(e)\n",
    "# you will see here the similar warning as in SimpleSelfAttention\n",
    "# where is the 'TODO' placeholder then?\n",
    "# and you can move on once your forward pass is ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c2b555-ce78-4126-8b23-59020d2d6df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.unsqueeze(0)  # Add batch dimension\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb16acc-7f17-4923-b63f-92ae384906b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_attention = CausalSelfAttention(input_dim, out_dim)\n",
    "output, attention_weights = causal_attention(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e6e20f-acde-4dae-aea7-68704e93f44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention weights\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(attention_weights[0].detach().numpy(), cmap='viridis')  # Convert to numpy if needed\n",
    "plt.title(\"Causal Self-Attention Weights\")\n",
    "plt.xlabel(\"Key\")\n",
    "plt.ylabel(\"Query\")\n",
    "plt.xticks(range(len(words)), words, rotation=45, ha='right')\n",
    "plt.yticks(range(len(words)), words)\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c3c34b-2ea3-4d07-8c86-ee6dfa6ca2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print transformed vectors for each word\n",
    "print(\"\\nTransformed word vectors:\")\n",
    "for i, word in enumerate(words):\n",
    "    print(f\"{word}: {output[0][i].detach().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1694a77b-e1e2-4089-beb5-018938af1399",
   "metadata": {},
   "source": [
    "#### Question to you"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4eeee4-8177-488c-b54a-88e7ac44a6b0",
   "metadata": {},
   "source": [
    "We now see how an autoregressive setting works with triangular mask. And that's the approach in decoder attention block. Thinking about encoder block and its setting of all tokens communicating with each other, what should we update in the current implementation of decoder to allow for this communication as in encoders? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1847ad28-34fd-4d14-b268-6b5a7bedc33f",
   "metadata": {},
   "source": [
    "## 5. Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fedf438-14fa-471f-9a23-0e9f8a9c5d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, seq_length, d_model = x.shape\n",
    "num_heads = 3  # We'll use 3 heads since d_model is 3\n",
    "head_dim = d_model // num_heads  # This will be 1 in this case\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a42dd0-04e1-4b0a-ae06-f01bf58a49b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear projections\n",
    "W_q = nn.Linear(d_model, d_model)\n",
    "W_k = nn.Linear(d_model, d_model)\n",
    "W_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "# Apply projections\n",
    "Q = W_q(x)\n",
    "K = W_k(x)\n",
    "V = W_v(x)\n",
    "\n",
    "print(f\"\\nShape after linear projection:\")\n",
    "print(f\"Q shape: {Q.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98db281a-fa09-44e0-8c9f-ee5deae44042",
   "metadata": {},
   "source": [
    "#### Question to you"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6046a25-d292-4860-b50f-02c22de66360",
   "metadata": {},
   "source": [
    "It won't be in our scope but have you thought of a \"proper\" weight initialization? If not but you want to think of it, check `xavier_uniform_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a73414-bf7e-488a-9166-de97c8cbe6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose to get dimensions [batch_size, num_heads, seq_len, head_dim]\n",
    "Q = Q.view(batch_size, seq_length, num_heads, head_dim).transpose(1, 2)\n",
    "K = K.view(batch_size, seq_length, num_heads, head_dim).transpose(1, 2)\n",
    "V = V.view(batch_size, seq_length, num_heads, head_dim).transpose(1, 2)\n",
    "\n",
    "print(f\"\\nShape after reshaping for multi-head:\")\n",
    "print(f\"Q shape: {Q.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ff0378-57b9-49f8-b8d6-4210e3499149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform attention for each head\n",
    "scores = torch.matmul(Q, K.transpose(-2, -1)) / (head_dim ** 0.5)\n",
    "attention_weights = torch.softmax(scores, dim=-1)\n",
    "head_outputs = torch.matmul(attention_weights, V)\n",
    "\n",
    "print(f\"\\nShape of each head's output:\")\n",
    "print(f\"head_outputs shape: {head_outputs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e8105c-bd7c-4fe8-a14e-d64ac6c9207b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose and concatenate heads [batch_size, seq_len, d_model]\n",
    "concat_heads = head_outputs.transpose(1, 2).contiguous().view(batch_size, seq_length, d_model)\n",
    "\n",
    "print(f\"\\nShape after concatenating heads:\")\n",
    "print(f\"concat_heads shape: {concat_heads.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5e91e4-02d9-45b0-997a-2d377a935afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project final output\n",
    "W_o = nn.Linear(d_model, d_model)\n",
    "output = W_o(concat_heads)\n",
    "\n",
    "print(f\"\\nFinal output shape:\")\n",
    "print(f\"output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4d914c-e20d-44a4-b8ef-87d4a9e91a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print attention weights for the first head\n",
    "print(f\"\\nAttention weights for the first head:\")\n",
    "print(attention_weights[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561e405f-1039-40de-8e08-67f737453756",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/multiattention.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model # the output dimensionality\n",
    "        self.num_heads = num_heads # the number of attention heads\n",
    "        self.head_dim = d_model // num_heads # dimensionality of each head output\n",
    "\n",
    "        # Making sure that each head will process a portion of the output\n",
    "        assert self.head_dim * num_heads == d_model, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        # Initialize weights for Q, K, V projections\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Output projection\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Scaling factor\n",
    "        #self.scale = self.head_dim ** -0.5\n",
    "        self.scale = 1 / math.sqrt(self.head_dim)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # TODO: Implement a multi-head attention class forward pass\n",
    "        \n",
    "        # Hint: 1. Apply linear projections to self.W_q, self.W_k and self.W_v\n",
    "        # 2. Use previously created CausalSelfAttention \n",
    "        # 3. Update attention scores using self.scale instead of math.sqrt(self.out_dim)\n",
    "        # 4. Concatenate heads with final output projection\n",
    "\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830b4fc3-edb3-46e5-9c46-3b86f77ecced",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.multiattention import MultiHeadAttention\n",
    "\n",
    "try:\n",
    "    check_implementation(MultiHeadAttention)\n",
    "except NotImplementedError as e:\n",
    "    print(e)\n",
    "# you can move on once your forward pass is ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d640efbb-52d6-48d0-b358-9faebd60727b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "output, attention_weights = mha(x, x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef6d8c3-784e-48a2-9c3e-226c182a2727",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86a83ad-21d1-48b4-8f7c-735f6f32e235",
   "metadata": {},
   "source": [
    "## 5. Attention Mechanism Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7380a754-7879-4c9b-92d4-7e2ae71e410a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(attention_weights, words=None, title=\"Attention Weights\"):\n",
    "    \"\"\"\n",
    "    Visualize attention weights as a heatmap.\n",
    "    \n",
    "    Parameters:\n",
    "    - attention_weights: torch.Tensor or numpy array of shape (seq_length, seq_length)\n",
    "    - words: list of words corresponding to the sequence (optional)\n",
    "    - title: title for the plot\n",
    "    \"\"\"\n",
    "    if isinstance(attention_weights, torch.Tensor):\n",
    "        attention_weights = attention_weights.detach().cpu().numpy()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))   \n",
    "    sns.heatmap(attention_weights, \n",
    "                annot=True, \n",
    "                cmap='viridis', \n",
    "                ax=ax, \n",
    "                cbar=True,\n",
    "                fmt='.2f')\n",
    "    \n",
    "    ax.set_title(title)\n",
    "    if words:\n",
    "        ax.set_xticks(range(len(words)))\n",
    "        ax.set_yticks(range(len(words)))\n",
    "        ax.set_xticklabels(words, rotation=45, ha='right')\n",
    "        ax.set_yticklabels(words, rotation=0)\n",
    "        ax.set_xlabel('Key')\n",
    "        ax.set_ylabel('Query')\n",
    "    else:\n",
    "        ax.set_xlabel('Sequence Position (Key)')\n",
    "        ax.set_ylabel('Sequence Position (Query)')\n",
    "    \n",
    "    # Adjust layout and display\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350dfab2-bc08-40d4-a8cf-b7e6b30181a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Experiment with the input\n",
    "# Not checking this but highly recommending\n",
    "\n",
    "sentence = \"Think of humanity on the path towards more unfathomable complexity\"\n",
    "words = sentence.split()\n",
    "\n",
    "# Create input tensor\n",
    "word_to_vec = {\n",
    "    \"Think\": [1, 0, 0], \n",
    "    \"of\": [0, 1, 0], \n",
    "    \"humanity\": [1, 0, 1],\n",
    "    \"on\": [0, 1, 1], \n",
    "    \"the\": [1, 1, 1], \n",
    "    \"path\": [1, 0, 0],\n",
    "    \"towards\": [1, 1, 0], \n",
    "    \"more\": [0, 0, 1], \n",
    "    \"unfathomable\": [1, 0, 1],\n",
    "    \"complexity\": [0, 1, 1]\n",
    "}\n",
    "\n",
    "x = torch.tensor([word_to_vec[word] for word in words], dtype=torch.float).unsqueeze(0)\n",
    "\n",
    "d_model = 3  # Dimension of our word vectors\n",
    "num_heads = 3  # We'll use 3 heads for simplicity, but you can increase this\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "# Get attention weights\n",
    "_, attention_weights = mha(x, x, x)\n",
    "\n",
    "# Visualize\n",
    "for i in range(num_heads):\n",
    "    visualize_attention(attention_weights[0, i], words, f\"Attention Weights - Head {i+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79b6fd4-17bd-40a8-850b-04a9c1581e97",
   "metadata": {},
   "source": [
    "#### Congratulations! You've implemented various attention mechanisms. These will be crucial components in building your LLM in the coming weeks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
