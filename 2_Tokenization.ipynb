{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61f84f1c-1b13-4a53-af09-6c81a49bbfe4",
   "metadata": {},
   "source": [
    "# Week 2: Turning Words into Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b923e5-bd9b-4c24-9324-5d7bf8999875",
   "metadata": {},
   "source": [
    "In this notebook, we will explore the process of converting text into tokens, a fundamental step in NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ca97c1-8bf4-4270-8963-97f31824d773",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8655f2-ae2d-49de-8add-d5bd2af993dc",
   "metadata": {},
   "source": [
    "We will begin by importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9560f8d8-0e98-485e-9d97-88f3838e1b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from typing import List, Dict, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e63d757-efa7-4008-9d53-6ad0aed6beb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import file checking whether TODO has been removed\n",
    "from helpers.check_todo import check_implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea13119-c573-47c5-876b-e9f866b88eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the src directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs('src', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa88576-c301-4f54-a63c-d6b2172419e7",
   "metadata": {},
   "source": [
    "## 1. Running Simple Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2145f93-25be-4d74-9a82-5fa3e11765e1",
   "metadata": {},
   "source": [
    "This section demonstrates a basic approach to tokenization using Python's built-in libraries and PyTorch. We will implement a basic tokenization function. This function will split the text into individual tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9009535b-4245-4033-9fe0-b431d7885cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"Hello, how are you doing today?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3518217b-5339-4983-89c7-3e8518f5984e",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_text = \"\"\"\n",
    "def calculate_llm_perplexity(model, text, max_length=1024):\n",
    "    tokens = tokenizer.encode(text, max_length=max_length, truncation=True)\n",
    "    input_ids = torch.tensor([tokens]).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "    loss = outputs.loss\n",
    "    return math.exp(loss.item())\n",
    "\n",
    "# Example usage\n",
    "perplexity = calculate_llm_perplexity(gpt2_model, \"Hello, world!\")\n",
    "print(f\"Perplexity: {perplexity:.2f}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb0ce8b-7b50-4b59-9882-a1c0003c2ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str) -> List[str]:\n",
    "    # TODO: Implement a basic tokenization function    \n",
    "    # Hint: Use regex to split the text into words and punctuation\n",
    "    pass  # Temporary placeholder to avoid syntax errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f66d39-42fc-43d2-ab00-10e9a2cb4148",
   "metadata": {},
   "source": [
    "Our time to test whether you have reviewed 'TODO' in the first function we implement together. Remove 'TODO' once you're done implementing and no error message will appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4f3c56-03dc-413b-85c8-e9aca0e33f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    check_implementation(tokenize)\n",
    "except NotImplementedError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286235de-79b4-4703-a4d6-95aef3c006af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tokenized text:\", tokenize(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bf3e36-e525-4b4f-a9e0-202ef052d52f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Tokenized code:\", tokenize(code_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148c43a7-3e71-487c-92bc-4a3f5376f919",
   "metadata": {},
   "source": [
    "## 2. Creating a Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f703890-f0b4-4d75-a1c4-37ea1ec4f12f",
   "metadata": {},
   "source": [
    "In this section we will create a function that takes a list of texts as input and returns a dictionary. In it each key is a unique word (or token) from the texts and its corresponding value is a unique index. The function should also reserve a special token <UNK> with index 0 to represent unknown words that may appear in future texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a448895f-faf6-496f-89d7-cb225e9c0734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(texts: List[str]) -> Dict[str, int]:\n",
    "\n",
    "    # TODO: Create a function to build a word-level vocabulary from a list of texts\n",
    "    # Hint: Use a set to collect unique tokens, then convert to a dictionary\n",
    "    # with enumerated indices\n",
    "    \n",
    "    # Do not forget to reserve a slot for unknown tokens as {'<UNK>': 0}\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3191793-955f-4e77-9c18-179e602157d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    check_implementation(build_vocabulary)\n",
    "except NotImplementedError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3c4f37-1dc6-48a2-bd9a-064c579587f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use your examples for a sample dataset\n",
    "# We won't be checking whether you have removed TODO here\n",
    "# But using your own sentences is encouraged!\n",
    "\n",
    "sample_dataset = [\n",
    "    \"42 is the Ultimate answer for Life, the Universe, and Everything.\",\n",
    "    \"Hello, world of LLM Trailblazers! This is another example.\",\n",
    "    \"What is the weather like today in Munich?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2e8bf6-d345-4607-aae9-592f06c5b7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocabulary(sample_dataset)\n",
    "print(\"Vocabulary:\", vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac055e16-0b54-40d9-9b8d-3bb05518c1ef",
   "metadata": {},
   "source": [
    "## 3. Implementing a Custom Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c078f7-0b94-416c-9333-60c7b7639cea",
   "metadata": {},
   "source": [
    "We have a lot of text data, but it's all different lengths. We need to make it work for our model. To do this, we'll create two special helpers:\n",
    "\n",
    "1. A `Dataset` class: This will help us prepare our text data for our model. We'll break down the text into smaller pieces and convert it into a format our model can understand.\n",
    "2. A `DataLoader` class: This will help us feed our prepared data to our model in batches. We'll sort the batches by length, add padding to make them all the same size, and create a mask to ignore the extra padding.\n",
    "\n",
    "By using these two helpers, we'll be able to get our data in order and make it easy for our model to work with. This will make our training process smoother and more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8ab068-e39a-4fa1-9563-35a52a34cd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts: List[str], vocab: Dict[str, int]):\n",
    "        \"\"\"\n",
    "        Initialize the dataset with texts and vocabulary.\n",
    "\n",
    "        :param texts: A list of text samples.\n",
    "        :param vocab: A dictionary representing the vocabulary, where keys are tokens and values are their corresponding IDs.\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.vocab = vocab\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        \n",
    "        # TODO: Return the number of samples in the input data\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
    "        \n",
    "        # TODO: Convert a text sample to token IDs using the vocabulary\n",
    "        # Hint: dictionary.get(keyname, value if a certain key doesn't exist) can be helpful\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6060a82-7c14-4038-95e1-156f4a27d6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    check_implementation(TextDataset)\n",
    "except NotImplementedError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17206567-8921-415a-a495-36524692cbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset instance\n",
    "dataset = TextDataset(sample_dataset, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91ce2dc-f5cf-43bb-bd8f-a8144e406cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "simple_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6813df26-8873-4452-900f-95ace96f6872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a batch of data\n",
    "for batch in simple_dataloader:\n",
    "    print(\"Batch shape:\", batch.shape)\n",
    "    print(\"Sample batch:\", batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824291b2-3280-4682-beaf-a5866a0067b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Attempting to iterate through the dataloader:\")\n",
    "try:\n",
    "    for batch in simple_dataloader:\n",
    "        print(\"Processed batch:\", batch)\n",
    "        break\n",
    "except RuntimeError as e:\n",
    "    print(f\"Caught an error: {e}\")\n",
    "    print(\"\\nThis error occurs because we're trying to batch sequences of different lengths.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03413975-cc74-40ca-9ab6-644b7610caa8",
   "metadata": {},
   "source": [
    "Now, let's implement a custom collate_fn to handle variable-length sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195014b6-4b41-4817-b11d-61daaab23d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: analyse this function and consider how to implement it without pad_sequence()\n",
    "def collate_fn(batch: List[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    # Separate the input sequences and targets\n",
    "    sequences, targets = zip(*batch)\n",
    "    \n",
    "    # Pad the sequences\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Pad the targets if they are sequences, otherwise just stack them\n",
    "    if isinstance(targets[0], torch.Tensor) and targets[0].dim() > 0:\n",
    "        padded_targets = pad_sequence(targets, batch_first=True, padding_value=0)\n",
    "    else:\n",
    "        padded_targets = torch.stack(targets)\n",
    "    \n",
    "    return padded_sequences, padded_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543f5e3a-6c54-4c1a-b641-5f11f695d6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad045f95-d5e9-4868-8645-c35831c5ed08",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Iterating through the dataloader with custom collate_fn:\")\n",
    "for batch, mask in dataloader:\n",
    "    print(\"Processed batch shape:\", batch.shape)\n",
    "    print(\"Mask shape:\", mask.shape)\n",
    "    print(\"Sample batch:\")\n",
    "    print(batch)\n",
    "    print(\"Sample mask:\")\n",
    "    print(mask)\n",
    "    break\n",
    "\n",
    "# TODO: Experiment with setting DataLoader with shuffle=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94cd25c-9415-407f-a010-b39478526306",
   "metadata": {},
   "source": [
    "The TextProcessor now successfully handles variable-length sequences!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776f9e7b-ddfb-4ec5-87aa-d28bf401179e",
   "metadata": {},
   "source": [
    "## 4. Putting It All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c8607e-fbaa-44a8-9767-77a718b9c98a",
   "metadata": {},
   "source": [
    "Time to combine tokenization, vocabulary creation and data preparation in batches. That's where our `TextProcessor` will help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cf6f66-3878-43ce-b02c-8977ac07b813",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProcessor:\n",
    "    def __init__(self):\n",
    "        self.vocab: Dict[str, int] = None\n",
    "    \n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "\n",
    "        # TODO: Implement tokenization\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def build_vocab(self, texts: List[str]) -> None:\n",
    "        \n",
    "        # TODO: Build vocabulary from a list of texts\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def create_dataloader(self, texts: List[str], batch_size: int) -> DataLoader:\n",
    "        \n",
    "        # TODO: Create a DataLoader with TextDataset from a list of texts\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61a1e65-2a3d-4e4c-93e4-17c3c8cf900d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    check_implementation(TextProcessor)\n",
    "except NotImplementedError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1215abd5-3e57-48ba-a4b3-071b34709563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the TextProcessor\n",
    "processor = TextProcessor()\n",
    "processor.build_vocab(sample_dataset)\n",
    "dataloader = processor.create_dataloader(sample_dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120a7934-9b3a-4193-85e1-c00ff9d0a433",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    print(\"Processed batch:\", batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6df659c-84cb-4c39-b0fb-1c3e77f61917",
   "metadata": {},
   "source": [
    "#### Congratulations! You've implemented a basic text processing pipeline. This will be useful for handling input data in your LLM projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1567c1-8a15-4868-a1bf-86bdf1d4c231",
   "metadata": {},
   "source": [
    "## Extra: Reviewing Tokenization Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6d4eb1-9c4c-476c-aa53-e7a627d57561",
   "metadata": {},
   "source": [
    "We'll use `tiktoken`at a later stage for tokenization, so let's see what it does and compare it to another simple tokenization library `NLTK`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0e5db2-4c03-4727-8855-89a294708764",
   "metadata": {},
   "source": [
    "### Using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ddffd0-0f5c-496e-b782-c27335494a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd232433-ce63-4249-a025-2d2b516cc011",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_tokens = word_tokenize(sample_text)\n",
    "print(\"NLTK Tokens:\", nltk_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c795464-0c53-4f8f-ba79-5c76bf276887",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_code_tokens = word_tokenize(code_text)\n",
    "print(\"NLTK Tokens for Code:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7ceb9e-0ba1-47cd-a438-5a043d68ab30",
   "metadata": {},
   "source": [
    "### Using Tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0a6973-2712-4577-91d6-a2a64fe40342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2422a0e7-b5de-4f53-a202-149fcc4cffea",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "tiktoken_tokens = enc.encode(sample_text)\n",
    "print(\"Tiktoken Tokens:\", tiktoken_tokens)\n",
    "print(\"Decoded Tiktoken Tokens:\", enc.decode(tiktoken_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b84397a-cee5-4dd5-99fa-d48bb261cb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"NLTK token count: {len(nltk_tokens)}\")\n",
    "print(f\"Tiktoken token count: {len(tiktoken_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056a7701-7cf8-4737-9b04-cb09c080b470",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiktoken_code_tokens = enc.encode(code_text)\n",
    "print(\"\\nTiktoken Tokens (decoded for readability):\")\n",
    "print(enc.decode_tokens_bytes(tiktoken_code_tokens))\n",
    "print(f\"Tiktoken token count: {len(tiktoken_code_tokens)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
