{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61f84f1c-1b13-4a53-af09-6c81a49bbfe4",
   "metadata": {},
   "source": [
    "# Week 2: Turning Words into Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b923e5-bd9b-4c24-9324-5d7bf8999875",
   "metadata": {},
   "source": [
    "In this notebook, we will explore the process of converting text into tokens, a fundamental step in NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ca97c1-8bf4-4270-8963-97f31824d773",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8655f2-ae2d-49de-8add-d5bd2af993dc",
   "metadata": {},
   "source": [
    "We will begin by importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9560f8d8-0e98-485e-9d97-88f3838e1b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from typing import List, Dict, Tuple, Union, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e63d757-efa7-4008-9d53-6ad0aed6beb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import file checking whether TODO has been removed\n",
    "from helpers.check_todo import check_implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ea13119-c573-47c5-876b-e9f866b88eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the src directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs('src', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa88576-c301-4f54-a63c-d6b2172419e7",
   "metadata": {},
   "source": [
    "## 1. Running Simple Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2145f93-25be-4d74-9a82-5fa3e11765e1",
   "metadata": {},
   "source": [
    "This section demonstrates a basic approach to tokenization using Python's built-in libraries and PyTorch. We will implement a basic tokenization function. This function will split the text into individual tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9009535b-4245-4033-9fe0-b431d7885cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"Hello, how are you doing today?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3518217b-5339-4983-89c7-3e8518f5984e",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_text = \"\"\"\n",
    "def calculate_llm_perplexity(model, text, max_length=1024):\n",
    "    tokens = tokenizer.encode(text, max_length=max_length, truncation=True)\n",
    "    input_ids = torch.tensor([tokens]).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "    loss = outputs.loss\n",
    "    return math.exp(loss.item())\n",
    "\n",
    "# Example usage\n",
    "perplexity = calculate_llm_perplexity(gpt2_model, \"Hello, world!\")\n",
    "print(f\"Perplexity: {perplexity:.2f}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5eb0ce8b-7b50-4b59-9882-a1c0003c2ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text.strip())\n",
    "    return [item.strip() for item in preprocessed if item.strip()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f66d39-42fc-43d2-ab00-10e9a2cb4148",
   "metadata": {},
   "source": [
    "Our time to test whether you have reviewed 'TODO' in the first function we implement together. Remove 'TODO' once you're done implementing and no error message will appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c4f3c56-03dc-413b-85c8-e9aca0e33f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m'tokenize' has been implemented. No TODO found.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    check_implementation(tokenize)\n",
    "except NotImplementedError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "286235de-79b4-4703-a4d6-95aef3c006af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized text: ['Hello', ',', 'how', 'are', 'you', 'doing', 'today', '?']\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenized text:\", tokenize(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63bf3e36-e525-4b4f-a9e0-202ef052d52f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized code: ['def', 'calculate', '_', 'llm', '_', 'perplexity', '(', 'model', ',', 'text', ',', 'max', '_', 'length=1024', ')', ':', 'tokens', '=', 'tokenizer', '.', 'encode', '(', 'text', ',', 'max', '_', 'length=max', '_', 'length', ',', 'truncation=True', ')', 'input', '_', 'ids', '=', 'torch', '.', 'tensor', '(', '[tokens]', ')', '.', 'to', '(', 'device', ')', 'with', 'torch', '.', 'no', '_', 'grad', '(', ')', ':', 'outputs', '=', 'model', '(', 'input', '_', 'ids', ',', 'labels=input', '_', 'ids', ')', 'loss', '=', 'outputs', '.', 'loss', 'return', 'math', '.', 'exp', '(', 'loss', '.', 'item', '(', ')', ')', '#', 'Example', 'usage', 'perplexity', '=', 'calculate', '_', 'llm', '_', 'perplexity', '(', 'gpt2', '_', 'model', ',', '\"', 'Hello', ',', 'world', '!', '\"', ')', 'print', '(', 'f', '\"', 'Perplexity', ':', '{perplexity', ':', '.', '2f}', '\"', ')']\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenized code:\", tokenize(code_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148c43a7-3e71-487c-92bc-4a3f5376f919",
   "metadata": {},
   "source": [
    "## 2. Creating a Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f703890-f0b4-4d75-a1c4-37ea1ec4f12f",
   "metadata": {},
   "source": [
    "In this section we will create a function that takes a list of texts as input and returns a dictionary. In it each key is a unique word (or token) from the texts and its corresponding value is a unique index. The function should also reserve a special token <UNK> with index 0 to represent unknown words that may appear in future texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a448895f-faf6-496f-89d7-cb225e9c0734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': 1,\n",
       " ',': 2,\n",
       " '.': 3,\n",
       " '42': 4,\n",
       " '?': 5,\n",
       " 'Everything': 6,\n",
       " 'Hello': 7,\n",
       " 'LLM': 8,\n",
       " 'Life': 9,\n",
       " 'Munich': 10,\n",
       " 'This': 11,\n",
       " 'Trailblazers': 12,\n",
       " 'Ultimate': 13,\n",
       " 'Universe': 14,\n",
       " 'What': 15,\n",
       " 'and': 16,\n",
       " 'another': 17,\n",
       " 'answer': 18,\n",
       " 'example': 19,\n",
       " 'for': 20,\n",
       " 'in': 21,\n",
       " 'is': 22,\n",
       " 'like': 23,\n",
       " 'of': 24,\n",
       " 'the': 25,\n",
       " 'today': 26,\n",
       " 'weather': 27,\n",
       " 'world': 28,\n",
       " '<UNK>': 0}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "def build_vocabulary(texts: List[str]) -> Dict[str, int]:\n",
    "\n",
    "    # TODO: Create a function to build a word-level vocabulary from a list of texts\n",
    "    # Hint: Use a set to collect unique tokens, then convert to a dictionary\n",
    "    # with enumerated indices\n",
    "    \n",
    "    # Do not forget to reserve a slot for unknown tokens as {'<UNK>': 0}\n",
    "    preprocessed  = itertools.chain(*[tokenize(text) for text in texts])\n",
    "    words = sorted(set(preprocessed))\n",
    "    vocab_size = len(words)\n",
    "    return {**{ token: i+1 for i, token in enumerate(words)}, **{'<UNK>': 0}}\n",
    "    \n",
    "build_vocabulary(sample_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3191793-955f-4e77-9c18-179e602157d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    check_implementation(build_vocabulary)\n",
    "except NotImplementedError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ca3c4f37-1dc6-48a2-bd9a-064c579587f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use your examples for a sample dataset\n",
    "# We won't be checking whether you have removed TODO here\n",
    "# But using your own sentences is encouraged!\n",
    "\n",
    "sample_dataset = [\n",
    "    \"42 is the Ultimate answer for Life, the Universe, and Everything.\",\n",
    "    \"Hello, world of LLM Trailblazers! This is another example.\",\n",
    "    \"What is the weather like today in Munich?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3c2e8bf6-d345-4607-aae9-592f06c5b7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'!': 1, ',': 2, '.': 3, '42': 4, '?': 5, 'Everything': 6, 'Hello': 7, 'LLM': 8, 'Life': 9, 'Munich': 10, 'This': 11, 'Trailblazers': 12, 'Ultimate': 13, 'Universe': 14, 'What': 15, 'and': 16, 'another': 17, 'answer': 18, 'example': 19, 'for': 20, 'in': 21, 'is': 22, 'like': 23, 'of': 24, 'the': 25, 'today': 26, 'weather': 27, 'world': 28, '<UNK>': 0}\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocabulary(sample_dataset)\n",
    "print(\"Vocabulary:\", vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac055e16-0b54-40d9-9b8d-3bb05518c1ef",
   "metadata": {},
   "source": [
    "## 3. Implementing a Custom Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c078f7-0b94-416c-9333-60c7b7639cea",
   "metadata": {},
   "source": [
    "We have a lot of text data, but it's all different lengths. We need to make it work for our model. To do this, we'll create two special helpers:\n",
    "\n",
    "1. A `Dataset` class: This will help us prepare our text data for our model. We'll break down the text into smaller pieces and convert it into a format our model can understand.\n",
    "2. A `DataLoader` class: This will help us feed our prepared data to our model in batches. We'll sort the batches by length, add padding to make them all the same size, and create a mask to ignore the extra padding.\n",
    "\n",
    "By using these two helpers, we'll be able to get our data in order and make it easy for our model to work with. This will make our training process smoother and more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2f8ab068-e39a-4fa1-9563-35a52a34cd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts: List[str], vocab: Dict[str, int]):\n",
    "        \"\"\"\n",
    "        Initialize the dataset with texts and vocabulary.\n",
    "\n",
    "        :param texts: A list of text samples.\n",
    "        :param vocab: A dictionary representing the vocabulary, where keys are tokens and values are their corresponding IDs.\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.vocab = vocab\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        \n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
    "        return torch.Tensor([self.vocab.get(item, \"<UNK>\") for item in self._tokenize(self.texts[idx])]).long()\n",
    "        \n",
    "    \n",
    "    def _tokenize(self, text: str) -> List[str]:\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text.strip())\n",
    "        return [item.strip() for item in preprocessed if item.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e6060a82-7c14-4038-95e1-156f4a27d6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mAll methods in class 'TextDataset' have been implemented. No TODO found.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    check_implementation(TextDataset)\n",
    "except NotImplementedError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "17206567-8921-415a-a495-36524692cbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset instance\n",
    "dataset = TextDataset(sample_dataset, vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a91ce2dc-f5cf-43bb-bd8f-a8144e406cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "simple_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8b0ccb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: torch.Size([1, 14])\n",
      "Sample batch: tensor([[ 4, 22, 25, 13, 18, 20,  9,  2, 25, 14,  2, 16,  6,  3]])\n"
     ]
    }
   ],
   "source": [
    "# Display a batch of data\n",
    "for batch in simple_dataloader:\n",
    "    print(\"Batch shape:\", batch.shape)\n",
    "    print(\"Sample batch:\", batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "824291b2-3280-4682-beaf-a5866a0067b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to iterate through the dataloader:\n",
      "Processed batch: tensor([[ 4, 22, 25, 13, 18, 20,  9,  2, 25, 14,  2, 16,  6,  3]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Attempting to iterate through the dataloader:\")\n",
    "try:\n",
    "    for batch in simple_dataloader:\n",
    "        print(\"Processed batch:\", batch)\n",
    "        break\n",
    "except RuntimeError as e:\n",
    "    print(f\"Caught an error: {e}\")\n",
    "    print(\"\\nThis error occurs because we're trying to batch sequences of different lengths.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03413975-cc74-40ca-9ab6-644b7610caa8",
   "metadata": {},
   "source": [
    "Now, let's implement a custom collate_fn to handle variable-length sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "195014b6-4b41-4817-b11d-61daaab23d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: analyse this function and consider how to implement it without pad_sequence()\n",
    "\n",
    "PAD_VALUE=0\n",
    "\n",
    "def pad_sequences(sequences: List[torch.Tensor], pad_value: int )->torch.Tensor:\n",
    "    \"\"\"\n",
    "    Pads list of sequences to same length; batch first\n",
    "    \"\"\"\n",
    "    max_len = max(seq.size(0) for seq in sequences)\n",
    "    padded_sequences = torch.full((len(sequences), max_len), pad_value)\n",
    "\n",
    "    for i, seq in enumerate(sequences):\n",
    "        padded_sequences[i, :seq.size(0)] = seq\n",
    "\n",
    "    return padded_sequences\n",
    "\n",
    "def collate_fn(\n",
    "        batch: Union[List[torch.Tensor], List[Tuple[torch.Tensor, torch.Tensor]]]\n",
    "    ) -> Union[torch.Tensor, Tuple[torch.Tensor, Optional[torch.Tensor]]]:\n",
    "    \n",
    "    print(batch)\n",
    "\n",
    "    # Separate the input sequences and targets\n",
    "    if isinstance(batch, tuple):\n",
    "        # Case where batch contains both sequences and targets\n",
    "        sequences, targets = zip(*batch)\n",
    "        \n",
    "        \n",
    "        # Pad the sequences\n",
    "        padded_sequences = pad_sequences(sequences, pad_value=PAD_VALUE)\n",
    "        \n",
    "        # Pad the targets if they are sequences, otherwise just stack them\n",
    "        if isinstance(targets[0], torch.Tensor) and targets[0].dim() > 0:\n",
    "            padded_targets = pad_sequences(targets, pad_value=PAD_VALUE)\n",
    "        else:\n",
    "            padded_targets = torch.stack(targets)\n",
    "        \n",
    "        return padded_sequences, padded_targets\n",
    "    else:\n",
    "        return pad_sequences(batch, pad_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "543f5e3a-6c54-4c1a-b641-5f11f695d6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ad045f95-d5e9-4868-8645-c35831c5ed08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating through the dataloader with custom collate_fn:\n",
      "[tensor([ 7,  2, 28, 24,  8, 12,  1, 11, 22, 17, 19,  3]), tensor([ 4, 22, 25, 13, 18, 20,  9,  2, 25, 14,  2, 16,  6,  3])]\n",
      "Batch shape: torch.Size([2, 14])\n",
      "Sample batch using the batch size of sequences and its value (1 in targets) or padding (0):\n",
      " tensor([[ 7,  2, 28, 24,  8, 12,  1, 11, 22, 17, 19,  3,  0,  0],\n",
      "        [ 4, 22, 25, 13, 18, 20,  9,  2, 25, 14,  2, 16,  6,  3]])\n"
     ]
    }
   ],
   "source": [
    "# Display a batch of data\n",
    "print(\"Iterating through the dataloader with custom collate_fn:\")\n",
    "for batch in dataloader:\n",
    "    print(\"Batch shape:\", batch.shape)\n",
    "    print(\"Sample batch using the batch size of sequences and its value (1 in targets) or padding (0):\\n\", batch)\n",
    "    break\n",
    "\n",
    "# TODO: Experiment with setting DataLoader with shuffle=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1cd39e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15, 22, 25, 27, 23, 26, 21, 10,  5,  0,  0,  0])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94cd25c-9415-407f-a010-b39478526306",
   "metadata": {},
   "source": [
    "The TextProcessor now successfully handles variable-length sequences!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776f9e7b-ddfb-4ec5-87aa-d28bf401179e",
   "metadata": {},
   "source": [
    "## 4. Putting It All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c8607e-fbaa-44a8-9767-77a718b9c98a",
   "metadata": {},
   "source": [
    "Time to combine tokenization, vocabulary creation and data preparation in batches. That's where our `TextProcessor` will help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "97cf6f66-3878-43ce-b02c-8977ac07b813",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProcessor:\n",
    "    def __init__(self):\n",
    "        self.vocab: Dict[str, int] = None\n",
    "    \n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text.strip())\n",
    "        return [item.strip() for item in preprocessed if item.strip()]\n",
    "    \n",
    "    def build_vocab(self, texts: List[str]) -> None:\n",
    "        \n",
    "        preprocessed  = itertools.chain(*[tokenize(text) for text in texts])\n",
    "        words = sorted(set(preprocessed))\n",
    "        vocab_size = len(words)\n",
    "        self.vocab = {**{ token: i+1 for i, token in enumerate(words)}, **{'<UNK>': 0}}\n",
    "        return self.vocab\n",
    "        \n",
    "    def create_dataloader(self, texts: List[str], batch_size: int) -> DataLoader:\n",
    "    \n",
    "        self.dataset = TextDataset(texts, self.vocab)\n",
    "        return DataLoader(self.dataset, batch_size, shuffle=True, collate_fn=self._collate_fn)\n",
    "\n",
    "    @staticmethod\n",
    "    def _pad_sequences(sequences: List[torch.Tensor], pad_value: int )->torch.Tensor:\n",
    "        \"\"\"\n",
    "        Pads list of sequences to same length; batch first\n",
    "        \"\"\"\n",
    "        max_len = max(seq.size(0) for seq in sequences)\n",
    "        padded_sequences = torch.full((len(sequences), max_len), pad_value)\n",
    "\n",
    "        for i, seq in enumerate(sequences):\n",
    "            padded_sequences[i, :seq.size(0)] = seq\n",
    "\n",
    "        return padded_sequences\n",
    "\n",
    "    @staticmethod\n",
    "    def _collate_fn(\n",
    "        batch: Union[List[torch.Tensor], List[Tuple[torch.Tensor, torch.Tensor]]]\n",
    "    ) -> Union[torch.Tensor, Tuple[torch.Tensor, Optional[torch.Tensor]]]:\n",
    "        # Separate the input sequences and targets\n",
    "        if isinstance(batch, tuple):\n",
    "            # Case where batch contains both sequences and targets\n",
    "            sequences, targets = zip(*batch)\n",
    "            \n",
    "            # Pad the sequences\n",
    "            padded_sequences = pad_sequences(sequences, pad_value=PAD_VALUE)\n",
    "            \n",
    "            # Pad the targets if they are sequences, otherwise just stack them\n",
    "            if isinstance(targets[0], torch.Tensor) and targets[0].dim() > 0:\n",
    "                padded_targets = pad_sequences(targets, pad_value=PAD_VALUE)\n",
    "            else:\n",
    "                padded_targets = torch.stack(targets)\n",
    "            \n",
    "            return padded_sequences, padded_targets\n",
    "        else:\n",
    "            return pad_sequences(batch, pad_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61a1e65-2a3d-4e4c-93e4-17c3c8cf900d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    check_implementation(TextProcessor)\n",
    "except NotImplementedError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1215abd5-3e57-48ba-a4b3-071b34709563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the TextProcessor\n",
    "processor = TextProcessor()\n",
    "processor.build_vocab(sample_dataset)\n",
    "dataloader = processor.create_dataloader(sample_dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "120a7934-9b3a-4193-85e1-c00ff9d0a433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch: tensor([[15, 22, 25, 27, 23, 26, 21, 10,  5,  0,  0,  0],\n",
      "        [ 7,  2, 28, 24,  8, 12,  1, 11, 22, 17, 19,  3]])\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    print(\"Processed batch:\", batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6df659c-84cb-4c39-b0fb-1c3e77f61917",
   "metadata": {},
   "source": [
    "#### Congratulations! You've implemented a basic text processing pipeline. This will be useful for handling input data in your LLM projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1567c1-8a15-4868-a1bf-86bdf1d4c231",
   "metadata": {},
   "source": [
    "## Extra: Reviewing Tokenization Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6d4eb1-9c4c-476c-aa53-e7a627d57561",
   "metadata": {},
   "source": [
    "We'll use `tiktoken`at a later stage for tokenization, so let's see what it does and compare it to another simple tokenization library `NLTK`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0e5db2-4c03-4727-8855-89a294708764",
   "metadata": {},
   "source": [
    "### Using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "63ddffd0-0f5c-496e-b782-c27335494a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/user/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "dd232433-ce63-4249-a025-2d2b516cc011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Tokens: ['Hello', ',', 'how', 'are', 'you', 'doing', 'today', '?']\n"
     ]
    }
   ],
   "source": [
    "nltk_tokens = word_tokenize(sample_text)\n",
    "print(\"NLTK Tokens:\", nltk_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "9c795464-0c53-4f8f-ba79-5c76bf276887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Tokens for Code:\n"
     ]
    }
   ],
   "source": [
    "nltk_code_tokens = word_tokenize(code_text)\n",
    "print(\"NLTK Tokens for Code:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7ceb9e-0ba1-47cd-a438-5a043d68ab30",
   "metadata": {},
   "source": [
    "### Using Tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "ad0a6973-2712-4577-91d6-a2a64fe40342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "2422a0e7-b5de-4f53-a202-149fcc4cffea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiktoken Tokens: [9906, 11, 1268, 527, 499, 3815, 3432, 30]\n",
      "Decoded Tiktoken Tokens: Hello, how are you doing today?\n"
     ]
    }
   ],
   "source": [
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "tiktoken_tokens = enc.encode(sample_text)\n",
    "print(\"Tiktoken Tokens:\", tiktoken_tokens)\n",
    "print(\"Decoded Tiktoken Tokens:\", enc.decode(tiktoken_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "5b84397a-cee5-4dd5-99fa-d48bb261cb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK token count: 8\n",
      "Tiktoken token count: 8\n"
     ]
    }
   ],
   "source": [
    "print(f\"NLTK token count: {len(nltk_tokens)}\")\n",
    "print(f\"Tiktoken token count: {len(tiktoken_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "056a7701-7cf8-4737-9b04-cb09c080b470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tiktoken Tokens (decoded for readability):\n",
      "[b'\\n', b'def', b' calculate', b'_ll', b'm', b'_per', b'plex', b'ity', b'(model', b',', b' text', b',', b' max', b'_length', b'=', b'102', b'4', b'):\\n', b'   ', b' tokens', b' =', b' tokenizer', b'.encode', b'(text', b',', b' max', b'_length', b'=max', b'_length', b',', b' trunc', b'ation', b'=True', b')\\n', b'   ', b' input', b'_ids', b' =', b' torch', b'.tensor', b'([', b'tokens', b']).', b'to', b'(device', b')\\n', b'   ', b' with', b' torch', b'.no', b'_grad', b'():\\n', b'       ', b' outputs', b' =', b' model', b'(input', b'_ids', b',', b' labels', b'=input', b'_ids', b')\\n', b'   ', b' loss', b' =', b' outputs', b'.loss', b'\\n', b'   ', b' return', b' math', b'.exp', b'(loss', b'.item', b'())\\n\\n', b'#', b' Example', b' usage', b'\\n', b'per', b'plex', b'ity', b' =', b' calculate', b'_ll', b'm', b'_per', b'plex', b'ity', b'(g', b'pt', b'2', b'_model', b',', b' \"', b'Hello', b',', b' world', b'!\")\\n', b'print', b'(f', b'\"', b'Per', b'plex', b'ity', b':', b' {', b'per', b'plex', b'ity', b':.', b'2', b'f', b'}\")\\n']\n",
      "Tiktoken token count: 115\n"
     ]
    }
   ],
   "source": [
    "tiktoken_code_tokens = enc.encode(code_text)\n",
    "print(\"\\nTiktoken Tokens (decoded for readability):\")\n",
    "print(enc.decode_tokens_bytes(tiktoken_code_tokens))\n",
    "print(f\"Tiktoken token count: {len(tiktoken_code_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6573e8d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65797e2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
